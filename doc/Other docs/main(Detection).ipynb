{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create dictionaries by word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean the data(Delete punctuations and irrelevent characteristics)\n",
    "def clean(words):\n",
    "    out = []\n",
    "    for c in words:\n",
    "        c = c.lower()\n",
    "        if c in set('abcdefghijklmnopqrstuvwxyz '):\n",
    "            out.append(c)\n",
    "    return ''.join(out)\n",
    "\n",
    "#Convert character to int\n",
    "def char_to_index(x):\n",
    "    return ord(x)-ord('a')\n",
    "\n",
    "#Combine all train_set files\n",
    "read_files = glob.glob(os.path.join(os.getcwd(), 'train_set', \"*.txt\"))\n",
    "with open(\"train_combine.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "#Clean the train_combine.txt file\n",
    "with open('train_combine.txt', encoding=\"utf8\") as file:\n",
    "    with open('train_clean.txt',\"w\") as file_clean:\n",
    "        for ws in file:\n",
    "            words=clean(ws)\n",
    "            file_clean.write(words)\n",
    "        file_clean.close()\n",
    "\n",
    "#Creat empty list and dictionary\n",
    "group_by_len = collections.defaultdict(list)\n",
    "digrams_by_len = collections.defaultdict(dict)\n",
    "\n",
    "#Group all words in tranning set by length\n",
    "with open('train_clean.txt', encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        for w in line.split():\n",
    "            group_by_len[len(w)].append(w)\n",
    "            \n",
    "#Create dictionaries by word length\n",
    "for length in group_by_len:\n",
    "        for i, j in itertools.combinations(range(length), 2):\n",
    "                key = (i, j)\n",
    "                matrix = [[0] * 26 for _ in range(26)]\n",
    "                for words in group_by_len[length]:\n",
    "                    matrix[char_to_index(words[i])][char_to_index(words[j])] = 1\n",
    "                    digrams_by_len[length][key] = matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the error detection list for test set based the dictionaryies created before\n",
    "#### * In the list, 0- the word is wrong, 1- the word is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 numbers of detection list: [0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Combine all test_set files\n",
    "read_files = glob.glob(os.path.join(os.getcwd(), 'test_set', \"*.txt\"))\n",
    "with open(\"test_combine.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "##Clean the test_combine.txt file\n",
    "with open('test_combine.txt', encoding=\"utf8\") as file:\n",
    "    with open('test_clean.txt',\"w\") as file_clean:\n",
    "        for ws in file:\n",
    "            words=clean(ws)\n",
    "            file_clean.write(words)\n",
    "        file_clean.close()\n",
    "        \n",
    "#Create the error detection list for test set based the dictionaryies created before\n",
    "Detection_list=[]\n",
    "with open('test_clean.txt', encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        for words in line.split():\n",
    "            list1=[]\n",
    "            if len(words)==1:\n",
    "                if words == \"a\" or words==\"i\":\n",
    "                    Detection_list.append(1)\n",
    "                else:\n",
    "                    Detection_list.append(0)\n",
    "            else:\n",
    "                for i, j in itertools.combinations(range(len(words)),2):\n",
    "                    key = (i, j) \n",
    "                    matrix=digrams_by_len[len(words)][key]\n",
    "                    if matrix[char_to_index(words[i])][char_to_index(words[j])] == 1:\n",
    "                        list1.append(1)\n",
    "                    else:\n",
    "                        list1.append(0)\n",
    "                if any(item == 0 for item in list1):\n",
    "                    Detection_list.append(0)\n",
    "                else:\n",
    "                    Detection_list.append(1)\n",
    "print(\"First 100 numbers in detection list:\",Detection_list[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct the Confusion matrix for the detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10123\n",
      "Error_rate0: 11.777150916784207 % (note:according to the method, the word is wrong, but actually it's right)\n",
      "Error_rate1: 27.363798545354744 % (note:according to the method, the word is right, but actually it's wrong)\n"
     ]
    }
   ],
   "source": [
    "#Combine all test_set_truth files\n",
    "read_files = glob.glob(os.path.join(os.getcwd(), 'test_set_truth', \"*.txt\"))\n",
    "with open(\"test_clean_truth.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "#Convert test_clean.txt and test_clean_truth.txt to list files\n",
    "wordlist=[]\n",
    "with open('test_clean.txt', encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        for words in line.split():\n",
    "            wordlist.append(words)   \n",
    "file_truth=[]          \n",
    "with open('test_clean_truth.txt', encoding=\"utf8\") as file1: \n",
    "    for line in file1:\n",
    "        for words in line.split():\n",
    "            file_truth.append(words)\n",
    "            \n",
    "#loop through numbers in detection_list and corresponding word in test_file to find false_detection or failed_detection rate       \n",
    "detection1=[]\n",
    "detection0=[]\n",
    "for i in range(len(Detection_list)):\n",
    "    if Detection_list[i]==1:\n",
    "        if wordlist[i] in file_truth:\n",
    "            detection1.append(1)\n",
    "        else:\n",
    "            detection1.append(0)\n",
    "    else:\n",
    "        if wordlist[i] in file_truth:\n",
    "            detection0.append(0)\n",
    "        else:\n",
    "            detection0.append(1)\n",
    "                \n",
    "error_rate0=(1-sum(detection0)/len(detection0))*100 #according to the method, the word is wrong, but actually it's right\n",
    "error_rate1=(1-sum(detection1)/len(detection1))*100 #according to the method, the word is right, but actually it's wrong\n",
    "print(\"Error_rate0:\",error_rate0,\"% (note:according to the method, the word is wrong, but actually it's right)\")\n",
    "print(\"Error_rate1:\",error_rate1,\"% (note:according to the method, the word is right, but actually it's wrong)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create one to one matching dataframe for Error word and Ground truth(Code in this chunk is made for the correction part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a=[]\n",
    "list_b=[] \n",
    "combine=pd.DataFrame()\n",
    "for path1,path2 in zip(glob.glob(os.path.join(os.getcwd(),'test_set','*.txt')),glob.glob(os.path.join(os.getcwd(),'test_truth','*.txt'))):\n",
    "    with open(path1, encoding=\"utf8\") as file1:\n",
    "        with open(path2, encoding=\"utf8\") as file2:\n",
    "            for line1,line2 in zip(file1,file2):\n",
    "                line3=clean(line1)\n",
    "                line4=clean(line2)\n",
    "                if len(line3)==len(line4):\n",
    "                    for word1,word2 in zip(line3.split(),line4.split()): \n",
    "                        if len(word1)!=1:\n",
    "                            for i, j in itertools.combinations(range(len(word1)),2):\n",
    "                                key = (i, j)\n",
    "                                list1=[]\n",
    "                                matrix=digrams_by_len[len(word1)][key]\n",
    "                                if matrix[char_to_index(word1[i])][char_to_index(word1[j])] == 1:\n",
    "                                    list1.append(1)\n",
    "                                else:\n",
    "                                    list1.append(0)\n",
    "                                if any(item == 0 for item in list1):\n",
    "                                    word_out= word1\n",
    "                                else:\n",
    "                                    word_out=\"Goooood\"\n",
    "                        list_a.append(word_out)\n",
    "                        list_b.append(word2)\n",
    "    matching = pd.DataFrame({\"Error\":list_a,\"Truth\":list_b})\n",
    "    matching = matching.drop(matching[matching.Error==\"Goooood\"].index)\n",
    "    matching = matching[matching.Error != matching.Truth]\n",
    "combine = pd.concat([combine,matching])\n",
    "#write code to excel\n",
    "writer = pd.ExcelWriter('Error_truth_total.xlsx')\n",
    "combine.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
