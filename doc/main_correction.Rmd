---
title: "correction"
author: "Lingyi Zhao"
date: "2018/11/24"
output:
  html_notebook:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
#install.packages("qualV")
#install.packages("PTXQC")
library(qualV)
library(PTXQC)
library(stringr)
library(quanteda)
library(readtext)
library(spacyr)
library(Rcpp)
library(magrittr)
library(dplyr)
#install.packages("tidytext")
library(tidytext)
#install.packages("janeaustenr")
library(janeaustenr)
#install.packages("randomForest")
library(randomForest)
#install.packages("stringr")
#install.packages("vecsets")
#install.packages("kableExtra")
library(stringr)
library(vecsets)
library(kableExtra)
```


###Combine data together


```{r, warning=FALSE}
#rm(list=ls())
setwd("/Users/lingyizhao/Documents/GitHub/Fall2018-Project4-sec2--sec2proj4_grp5")
data<-read.table('/Users/lingyizhao/Documents/GitHub/Fall2018-Project4-sec2--sec2proj4_grp5/output/Correction_output/test_clean.txt')
data<-t(data)
data<-data.frame(data)
tesseract_vec<-read.csv('~/output/Correction_output/Detection_list.csv', header = FALSE)
data[,2]<-tesseract_vec[,1]


errorwords<-matrix(data[data$V2==0,1])
names(errorwords)<-"error"
```

###Candidate dictionary:

For the total candidate set, we want to find a small specific subset for every error word. Here, we use minimum edit distance to calculate every error word with each word in candidate set. We choose threshold to 20 since we believe top 20 minimum edit distance are enough for our each error???s candidate. For example, if we choose threshold is equal to 20, this means we can accept at least 0 edit distance and at most around 3 or 4 characters different with error word. 


```{r, warning=FALSE}
candidate<-read.table('~/output/Correction_output/test_clean_truth.txt')
candidate<-t(candidate)
candidate<-data.frame(candidate)

#data[,3]<-candidate[,1]
#names(data)<-c("error","list","truth")
```

###Candidate Search:

##We choose candidate from this part:

```{r, warning=FALSE}
candidate_error<-data.frame(NA, nrow=20)
candidate_errors<-data.frame(NA, nrow=20)
#nrow(candidate_errors)<-20

distances<-data.frame()
findcandidates_20<-data.frame()
  threshold_c<-20
for (j in 1:nrow(errorwords)){
  for (i in 1:nrow(candidate)){
    distances[i,j]<-adist(errorwords[j],candidate[i,1])
    
  }
}
save(distances, file = "distances.RData")

candidate<-data.frame(candidate)
row.names(candidate)<-c(1:nrow(candidate))
for (p in 1:ncol(distances)){
  candidate_order<-data.frame(candidate[order(distances[,p], decreasing = FALSE),1])
  candidate_error<-candidate_order[1:threshold_c,1]
  candidate_errors<-data.frame(candidate_errors, candidate_error)
}
candidate_errors<-candidate_errors[,c(-1,-2)]
candidate_errors<-t(candidate_errors)
row.names(candidate_errors)<-errorwords[,1]
colnames(candidate_errors)<-c(1:20)

save(candidate_errors, file = "candidate_errors.RData")
  ######here, candidate_error is the small candidate set for our error words, every error word with top 20 candidates. We will use this set for the feature scoring part. 
  ##### The row is error word, columns are there candidates. I will change name of column to the errorword, so you can find every error words with their top20 candidates by using column names
  ##### also this will need long time to run, becasue this is a data.frame with 10104 rows and 2000+ columns. So I'll run this and then I provide guys the .RData. 

load("/Users/lingyizhao/Documents/GitHub/Fall2018-Project4-sec2--sec2proj4_grp5/data/candidate_errors.RData")
```


###Feature Scoring:

##Levenshtein edit distance:

In this feature scoring, we want to calculate the difference between two strings in spell correction. We use threshold here is 100, since we want to make every score within 0 to 1. 


```{r, warning=FALSE}
#library(stringr)
threshold<-100
score_L<-data.frame()
dists<-0
for (j in 1:length(errorwords)){
  for (i in 1:ncol(candidate_errors)){
    #characters<-min(nchar(candidate[i,1]),nchar(errorwords[j]))
    # str_c<-strsplit(candidate[i,1], "")
    # str_e<-strsplit(errorwords[j], "")
    # for (k in characters){
    #   char_c_check<-str_c[[1]][k]
    #   char_e_check<-str_e[[1]][k]
    #   if(char_c_check!=char_e_check){
    #     dists=dists+1+(nchar(candidate[i,1])-nchar(errorwords[j]))
    #   }
    # }
    
    dists<-adist(errorwords[j],candidate_errors[1,i])
    score_L[i,j]<-1-(dists/(threshold+1))
  }
}

score_L<-t(score_L)
row.names(score_L)<-errorwords[,1]
##every error word with their top20 candidates
save(score_L, file = "score_L.RData")
score<-load("/Users/lingyizhao/Documents/GitHub/Fall2018-Project4-sec2--sec2proj4_grp5/data/score_L.RData")
variable1<-score_L
```

##String Similarity:

Longest common subsequence is an alternative approach than first one. Here we use three method for our calculation, subsequence from 1st character, subsequence from middle character and subsequence ending at last character. Notice, we also normalized our score inthsi method. 


```{r, warning=FALSE}

nlcs<- function(str1, str2,index) {
    l1 <- nchar(str1)
    l2 <- nchar(str2)
    str1 <- unlist(strsplit(str1,""))
    #print(str1)
    str2 <- unlist(strsplit(str2,""))
    if(index == 1){
      l1 <- l1-1
      l2 <- l2-1
      str1 <- str1[-1]
      str2 <- str2[-1]
    }
  
    nlcs_value <- (((LCS(str1, str2)$LLCS)^2)*2)/(l1+l2)
    return(nlcs_value)
}
#nlcs("word","mother",1)

score2<-data.frame()

for (j in 1:length(errorwords)){
  for (i in 1:nrow(candidate_errors)){
#    for (j in 1:10){
#  for (i in 1:10){
    nlcs_value<-nlcs(errorwords[j],candidate_errors[i,1],0)
    nmnlcs1 <- nlcs(errorwords[j],candidate_errors[i,1],1)
    n<- ceiling(nchar(errorwords[j])/2)
    nmnlcsn <- nlcs(errorwords[j],candidate_errors[i,1],n)
    z <- nchar(errorwords[j])
    nmnlcsz <- nlcs(errorwords[j],candidate_errors [i,1],z)
    score2[i,j]<-0.25*nlcs_value+0.25*nmnlcs1+0.25*nmnlcsn+0.25*nmnlcsz
  }
}


variable2<-load("~/score_SM.RData")
```

##Language popularity:

We check the frequency of each candidate for each error in the ground truth and normalized every score within candidate and error. 


```{r, warning=FALSE}
##load candidate data
candidate_errors<-as.data.frame(candidate_errors)
head(candidate_errors)
##load clean text data
data_clean<-candidate
##remove NA
data_clean<-data_clean[!is.na(data_clean)]
#candidate_errors
#data_clean=="as"
##function to calculate the frequency
freq<-function(word){
  number<-sum(data_clean==word)
  return(number)
}

##frequency
freq_table<-c()
for (i in 1:nrow(candidate_errors)){
  freq_table<-rbind(freq_table,apply(candidate_errors[i,],2,freq))
}

##frequency/max
freq_table<-as.data.frame(freq_table)
max_frequency<-apply(freq_table,1,max)
freq_table$maxfreq<-max_frequency

##function to calculate porpotion
freq_score<-c()
for ( i in 1:20){
  freq_score<-cbind(freq_score,freq_table[,i]/freq_table[,21])
}
row.names(freq_score)<-row.names(candidate_errors)
##score table
variable3<-freq_score

```

##Lexion existance:

In this method, we want to use a domain specific lexicon to capture terminologies. For example, if this candidate of first error appeared in the lexicon, we give score for the candidate and the error to 1. Otherwise, we give the score to 0. 


```{r, warning=FALSE}

#setwd('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5/data')
load('candidate_errors.RData')

matchfun<-function(c){
  if(c%in%tk4){
    return(1)
  }else{
    return(0)
  }
}


#lexicon for group 1:
f<-readtext('~/ground_truth/group1/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)

exist1<-data.frame(matrix(nrow=nrow(candidate_errors),ncol=ncol(candidate_errors)))

for(i in 1:nrow(candidate_errors)){
exist1[i,]<-as.data.frame(lapply(candidate_errors[i,],matchfun))
}

#lexicon for group 2:
f<-readtext('~/ground_truth/group2/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)

exist2<-data.frame(matrix(nrow=nrow(candidate_errors),ncol=ncol(candidate_errors)))

for(i in 1:nrow(candidate_errors)){
  exist2[i,]<-as.data.frame(lapply(candidate_errors[i,],matchfun))
}

#lexicon for group 3:
f<-readtext('~/ground_truth/group3/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)

exist3<-data.frame(matrix(nrow=nrow(candidate_errors),ncol=ncol(candidate_errors)))

for(i in 1:nrow(candidate_errors)){
  exist3[i,]<-as.data.frame(lapply(candidate_errors[i,],matchfun))
}

#lexicon for group 4:
f<-readtext('~/ground_truth/group4/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)

exist4<-data.frame(matrix(nrow=nrow(candidate_errors),ncol=ncol(candidate_errors)))

for(i in 1:nrow(candidate_errors)){
  exist4[i,]<-as.data.frame(lapply(candidate_errors[i,],matchfun))
}

#lexicon for group 5:
f<-readtext('~/ground_truth/group5/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)

exist5<-data.frame(matrix(nrow=nrow(candidate_errors),ncol=ncol(candidate_errors)))

for(i in 1:nrow(candidate_errors)){
  exist5[i,]<-as.data.frame(lapply(candidate_errors[i,],matchfun))
}

#storing it in a list and in an RData file
#all_grps$group1
save(all_grps, file = "lexicon_existance.RData")
lexicon_existance <- load("lexicon_existance.RData")
variable4 <- all_grps$group1
```

##Exact-context popularity:

This feature scoring method used context for our candidates since approach correction should be coherent in context. First of all, we find the context of every error in tesseract and make sure there have 4 words before error and 4 words after words (the total words should be 9 words).  And then we need to replace error with their each candidate and construct 5-gram of these small ???sentence???. Finally, we calculate the frequency of each small ???sentence??? and give the score for each error and each its candidate. 


```{r, warning=FALSE}


group=5
index_e<-data.frame(errorwords)
g<-1
  for (c in 1:nrow(data)){
    #print(c)
    if (data[c,2]==0){
      index_e[g,2]<-c
      print(index_e[g,2])
      if (c<5){
        index1<-index_e[g,2]
        index9<-index_e[g,2]+(group-1)
        index_e[g,3]<-paste(data[index1:index9,1], collapse = " ")
  }
    if(c>10119){
      index1<-index_e[g,2]-(group-1)
      index9<-index_e[g,2]
      index_e[g,3]<-paste(data[index1:index9,1], collapse = " ")
   }
    if(c>=5 && c<=10119){
      index1<-index_e[g,2]-(group-1)
      index9<-index_e[g,2]+(group-1)
      index_e[g,3]<-paste(data[index1:index9,1], collapse = " ")
    }
      g<-g+1
    }
    
  }
  # index_e[g,2]<-which(data[,1]==errorwords[g])
  # print(g)
  

candidate_replace_error<-data.frame()
#row_total<-threshold_c*nrow(errorwords)
a<-1
for (b in 1:nrow(errorwords)){
  for (d in 1:(ncol(candidate_errors))){
    candidate_replace_error[a,1]<-gsub(index_e[b,1],candidate_errors[b,d],index_e[b,3] )
    a<-a+1
  }
}

freq<-data.frame()
i<-1
total_context<-threshold_c*length(errorwords)
for (r in 1:nrow(candidate_replace_error)){
  #print(r)
  if(r<41 || r>47460){
    b<-as.data.frame(gregexpr(candidate_replace_error[r,1], candida))
    freq[i,1]<-candidate_replace_error[r,1]
    if(b[,1]==-1){
      c<-0
      freq[i,2]<-c
    }else{
    c<-count(b[,1])
    freq[i,2]<-sum(c$freq)
    }
    #freq[i,1]<-candidate_replace_error[r,1]
    #freq[i,2]<-sum(c$freq)
    i<-i+1
  }
  if(r>=41 && r<=47460){
    te<-candidate_replace_error[r,1]
    a<-austen_books()%>%
      unnest_tokens(ngram, te, token="ngrams", n=5)
    freq[i:(i+4),1]<-a$ngram[1:5]
    # for (s in 1:5){
    #   b<-as.data.frame(gregexpr(freq[i,1], candida))
    #   c<-count(b[,1])
    # }
    b1<-as.data.frame(gregexpr(freq[i,1], candida))
    if(b1[,1]==-1){
      c1<-0
      freq[i,2]<-c1
    }else{
    c1<-count(b1)
    freq[i,2]<-sum(c1$n)
    }
    #c1<-count(b1[,1])
    #freq[i,2]<-sum(c1$freq)
    
    b2<-as.data.frame(gregexpr(freq[i+1,1], candida))
    if(b2[,1]==-1){
      c2<-0
      freq[i+1,2]<-c2
    }else{
    c2<-count(b2)
    freq[i+1,2]<-sum(c2$n)
    }
    #c2<-count(b2[,1])
    #freq[i+1,2]<-sum(c2$freq)
    b3<-as.data.frame(gregexpr(freq[i+2,1], candida))
    if(b3[,1]==-1){
      c3<-0
      freq[i+2,2]<-c3
    }else{
    c3<-count(b3)
    freq[i+2,2]<-sum(c3$n)
    }
    #c3<-count(b3[,1])
    #freq[i+2,2]<-sum(c3$freq)
    b4<-as.data.frame(gregexpr(freq[i+3,1], candida))
    if(b4[,1]==-1){
      c4<-0
      freq[i+3,2]<-c4
    }else{
    c4<-count(b4)
    freq[i+3,2]<-sum(c4$n)
    }
    #c4<-count(b4[,1])
    #freq[i+3,2]<-sum(c4$freq)
    b5<-as.data.frame(gregexpr(freq[i+4,1], candida))
    if(b5[,1]==-1){
      c5<-0
      freq[i+4,2]<-c5
    }else{
    c5<-count(b5)
    freq[i+4,2]<-sum(c5$n)
    }
    #c5<-count(b5[,1])
    #freq[i+4,2]<-sum(c5$freq)
    i<-i+5
  }
  
}

#save(freq, file = "freq.RData")
freq1<-load("~/output/Correction_output/freq_to_30001.RData")#30000
freq1<-freq
freq2<-load("~/output/Correction_output/freq_30000_31050.RData")#31050
freq2<-freq[-c(5256:5260),]
freq3<-load("~/output/Correction_output/freq_35000.RData")#35000
freq3<-freq
freq4<-load("~/output/Correction_output/freq_35000_38002.RData")#38002
freq[15005:15010,2]<-0
freq4<-freq
freq5<-load("~/output/Correction_output/freq_38003_39999.RData")
freq5<-freq
freq6<-load("~/output/Correction_output/freq_40000_47500.RData")
freq6<-freq
freq_total<-rbind(freq1, freq2, freq3, freq4, freq5, freq6)

score_Econtext<-data.frame()
w<-41
score_Econtext<-rbind((freq_total[1:20,2]), (freq_total[21:40,2]))
score_Econtext<-data.frame(score_Econtext)
c<-41

for (p in 3:(nrow(errorwords)-2)){
  #w<-c
  for (q in 1:20){
    score_Econtext[p,q]<-sum(freq_total[w:(w+4), 2])
    w<-w+5
    #c<-w
    #print(w)
  }
  
}

save(freq_total, file = "~/output/Correction_output/freq_total.RData")
score_Econtext[(nrow(errorwords)-1),1:20]<-freq[(nrow(freq)-40+1):(nrow(freq)-20),2]
score_Econtext[nrow(errorwords), 1:20]<-freq[(nrow(freq)-20+1):(nrow(freq)),2]

save(score_Econtext, file = "~/output/Correction_output/score_Econtext.RData")

variable5<-score_Econtext
```


###Regression model: random forest:

```{r, warning=FALSE}

set.seed(100)
score_sort<-data.frame()

library(readxl)
library(randomForest)
error_truth <- read_excel('~/output/Dectection_output/Error_truth_total.xlsx')
#error_truth<-read.csv("Error_truth_total.csv")

total_final_data<-data.frame()
total_final_data<-data.frame(rep(errorwords, each=5))
i<-1
for (k in 1:nrow(candidate_errors)){
  total_final_data[i:(i+4),2]<-matrix(t(candidate_errors[k, 1:5]))
  total_final_data[i:(i+4),3]<-matrix(t(variable1[k, 1:5]))
  total_final_data[i:(i+4),4]<-matrix(t(variable2[k, 1:5]))
  total_final_data[i:(i+4),5]<-matrix(t(variable3[k, 1:5]))
  total_final_data[i:(i+4),6]<-matrix(t(variable4[k, 1:5]))
  total_final_data[i:(i+4),7]<-matrix(t(variable5[k, 1:5]))
  i<-i+5
}

#install.packages("prodlim")
library(prodlim)
a <- matrix(row.match(total_final_data[,1:2],error_truth[,2:3]))
x1 <- replace(a,is.na(a),0)
x1[x1>0] = 1

total_final_data$V5 <- x1

number<-sample(1:nrow(total_final_data), nrow(total_final_data)*0.8)
trainset<-total_final_data[1:9500, ]
testset <- total_final_data[9501:11875, ]
model<-randomForest(trainset[,3:4], y=trainset$V5, ntree=100)
pred <- predict(model, testset[,3:4])
pred<-matrix(pred)
total_final_data<-cbind(total_final_data ,pred)
# dim(label)
# dim(trainset)
# model

```

###Correct text:

```{r, warning=FALSE}
require(data.table) ## 1.9.2
group <- as.data.table(total_final_data)
df2<-group[group[, .I[which.max(pred)], by=total_final_data$rep.errorwords..each...5.]$V1]
  df2<-df2[,c(-7)]
  corrected<-df2[,c(1,2)]  
  colnames(corrected)<-c('error','ChosenCandidate') 
  
test_cleaned<-read.table('~/output/Detection_output/test_clean.txt',header=FALSE,sep="\t")

library(qdap)
tclean<-t(test_cleaned)
t_cleaned <- mgsub(as.character(corrected$error), corrected$ChosenCandidate, as.character(tclean))

write.table(t_cleaned, '~/output/Correction_output/corrected_text.txt', append = FALSE, sep = " ",
            col.names = FALSE)

for(j in seq_along(corrected$error)){
  t_cleaned <- gsub(as.character(corrected$error[j]), corrected$ChosenCandidate[j], as.character(tclean))
}
```


###Evaluation: 

```{r, warning=FALSE}
ifCleanToken <- function(cur_token){
  now <- 1
  if_clean <- TRUE
  
  ## in order to accelerate the computation, conduct ealy stopping
  rule_list <- c("str_count(cur_token, pattern = '[A-Za-z0-9]') <= 0.5*nchar(cur_token)", # If the number of punctuation characters in a string is greater than the number of alphanumeric characters, it is garbage
                 "length(unique(strsplit(gsub('[A-Za-z0-9]','',substr(cur_token, 2, nchar(cur_token)-1)),'')[[1]]))>1", #Ignoring the first and last characters in a string, if there are two or more different punctuation characters in thestring, it is garbage
                 "nchar(cur_token)>20") #A string composed of more than 20 symbols is garbage 
  while((if_clean == TRUE)&now<=length(rule_list)){
    if(eval(parse(text = rule_list[now]))){
      if_clean <- FALSE
    }
    now <- now + 1
  }
  return(if_clean)
}



## read the ground truth text
current_ground_truth_txt <- readLines("~/Desktop/test_clean_truth.txt", warn=FALSE)

## read the tesseract text
current_tesseract_txt <- readLines("~/output/Detection_output/test_clean.txt", warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")

## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] 
#tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
tesseract_delete_error_vec <- tesseract_vec[tesseract_if_clean]

## postprocessing text
tesseract_post_txt <- readLines("~/output/Correction_output/corrected_text_latest.txt", warn=FALSE)
tesseract_clean_post_txt <- paste(tesseract_post_txt, collapse = " ")


```

```{r, warning=FALSE}
##string split the text file
#ground truth
ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]] 
ground_truth_char<-unlist(strsplit(ground_truth_vec,"",fixed = TRUE))
#original ocr file
tesseract_char<-unlist(strsplit(tesseract_vec,"",fixed = TRUE))
#postprocessing file
tesseract_post_vec <- str_split(tesseract_clean_post_txt," ")[[1]] 
tesseract_post_char<-unlist(strsplit(tesseract_clean_post_txt,"",fixed = TRUE))


## Here, we compare the lower case version of the tokens
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec)) 
old_intersect_char <- vecsets::vintersect(tolower(ground_truth_char), tolower(tesseract_char)) 
```

```{r}
new_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_post_vec)) 
new_intersect_char <- vecsets::vintersect(tolower(ground_truth_char), tolower(tesseract_post_char)) 
##postprocessing vectors

OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",
                                      "character_wise_recall","character_wise_precision")
##word evaluation
OCR_performance_table["word_wise_recall","Tesseract"] <- length(old_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract"] <- length(old_intersect_vec)/length(tesseract_vec)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(tesseract_vec)
kable(OCR_performance_table, caption="Summary of OCR performance")
##character evaluation
OCR_performance_table["character_wise_recall","Tesseract"] <- length(old_intersect_char)/length(ground_truth_char)
OCR_performance_table["character_wise_precision","Tesseract"] <- length(old_intersect_char)/length(tesseract_char)
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_char)/length(ground_truth_char)
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_char)/length(tesseract_char)
kable(OCR_performance_table, caption="Summary of OCR performance-character evaluation")
```

