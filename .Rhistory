ind<-sample(1:nrow(df62[df62$region==reg[i],]),n_h[i],replace=FALSE)
cc=rbind(cc,df6[ind,])
}
for(i in 1:4){
ind<-sample(1:nrow(df62[df62$region==reg[i],]),n_h[i],replace=FALSE)
cc=rbind(cc,df62[ind,])
}
View(cc)
#check how many rows for 'NC'
nrow(cc[cc$region=='NC',])
sample(which(df62[df62$region=='NC',]),75,replace=FALSE)
which(df62[df62$region=='NC',])
which(df62$region=='NC')
sample(which(df62$region=='NC'),75,replace=FALSE)
df<-agstrat
df4<-df[,c(3,15)]
df4<-data.frame(list(agstrat$acres92)[[1]],list(agstrat$region)[[1]])
colnames(df4)<-c('area','region')
mn<-aggregate(df4,list(df4$region),mean)
s_h<-aggregate(df4,list(df4$region),sd)
n_h<-c(103,21,135,41)
N_h<-c(1054,220,1382,422)
est_var<-(1-n_h/N_h)*(N_h^2)*(s_h$area^2/n_h)
est_tots<-mn$area*N_h
tots<-sum(est_tots)
var_tots<-sum(est_var)
se<-sqrt(var_tots)
#CI:
CI4<-c(tots-1.96*se,tots+1.96*se)
#The Stratified gives a higher confidence interval than SRS
#QUESTION 5:
df5<-data.frame(Nh=N_h,sh=sqrt(est_var))
df5$Nhsh<-df5$Nh*df5$sh
df5$nh<-df5$Nhsh*(300/sum(df5$Nhsh))
nh<-floor(df5$nh)
n_h<-c(75,4,151,70)
reg<-c('NC','NE','S','W')
sum(nh)
#{NC,NE,S,W}
#{75,4,151,70=300}
#QUESTION 6:
df<-agpop
df<-df[order(df$region),]
set.seed(5234)
df62<-df[,c(3,15)]
cc<-data.frame(ar=c(),reg=c())
for(i in 1:4){
ind<-sample(which(df62$region==reg[i]),n_h[i],replace=FALSE)
cc=rbind(cc,df62[ind,])
}
#check how many rows for 'NC'
nrow(cc[cc$region=='NC',])
df62<-cc[order(cc$region),]
mn6<-aggregate(df62,list(df62$region),mean)
s_h6<-aggregate(df62,list(df62$region),sd)
est_tots6<-mn6$acres92*N_h
est_var6<-(1-n_h/N_h)*(N_h^2)*(s_h6$acres92^2)/(n_h)
tots6<-sum(est_tots6)
se6<-sqrt(sum(est_var6))
#CI6:
CI6<-c(tots6-1.96*se6,tots6+1.96*se6)
se6/se
df7<-otters
View(df7)
length(df7$habitat==1)
length(df7$habitat==2)
length(df7$habitat[df$habitat==2])
n_h<-c(19,20,22,21)
mean7<-aggregate(df7,list(df7$habitat),mean)
s_h7<-aggregate(df7,list(df7$habitat),sd)
N_h<-c(89,61,40,47)
View(df7)
View(s_h7)
View(mean7)
est_tots<-(1-n_h/N_h)*(N_h^2)*(s_h7$holts^2)/n_h
est_var<-(1-n_h/N_h)*(N_h^2)*(s_h7$holts^2)/n_h
est_tots<-N_h*mean7$holts
est_var7<-(1-n_h/N_h)*(N_h^2)*(s_h7$holts^2)/n_h
est_tots7<-N_h*mean7$holts
tots7<-sum(est_tots7)
se7<-sqrt(sum(est_var7))
#CI:
CI7<-c(tots7-1.96*se7,tots7+1.96*se7)
library(mitools)
library(survey)
case=rep(seq(1,12,1),each=3);
can=seq(1,36,1);
frg=c(1,5,7,4,2,4,0,1,2,3,6,6,4,9,8,0,7,3,5,5,1,3,0,2,7,3,5,3,1,4,4,7,9,0,0,0)
#Cluster sizes
csize=rep(24,36)
noclusters=rep(580,36)
# Sampling weights
N=580
n=12;
mi=rep(3,36)
wght=(N*csize)/(n*mi)
corndf=data.frame(case,can,csize,noclusters,wght)
corn.svy=svydesign(id=~case+can,weights=wght,data=corndf,fpc=~noclusters+csize)
svymean(frg,design=corn.svy)
m<-3.64
corn.svy$variables
corn.svy$cluster
M_i<-12
#Total:
t<-M_i*m
View(corndf)
corn.svy
corn.svy$variables
mfrg<-mean(frg)
si<-sd(frg)
knitr::opts_chunk$set(echo = TRUE)
c1<-c(1,4,0,3,4,0,5,3,7,3,4,0)
c2<-c(5,2,1,6,9,7,5,0,3,1,7,0)
c3<-c(7,4,2,6,8,3,1,2,5,4,9,0)
df<-data.frame(c1=c1,c2=c2,c3=c3)
View(df)
df$yi_bar<-apply(df,1,mean)
N=580
n=12
Mi=24
mi=3
for(i in 1:nrow(df)){
df$si_sq[i]<-(sum(df[i,c(1,2,3)]-df[i,4]))^2/mi-1
}
for(i in 1:nrow(df)){
df$si_sq[i]<-(sum(df[i,]-df[i,4]))^2/mi-1
}
df[1,c(1:3)]
df[1,4]
for(i in 1:nrow(df)){
yij<-df[i,c(1:3)]
df$si_sq[i]<-sum((yij-df[i,4])^2)/mi-1
}
apply(df[,1:3],1,sd)
for(i in 1:nrow(df)){
yij<-df[i,c(1:3)]
df$si_sq[i]<-sum((yij-df[i,4])^2)/mi
}
for(i in 1:nrow(df)){
yij<-df[i,c(1:3)]
df$si_sq[i]<-sum((yij-df[i,4])^2)/mi-1
}
for(i in 1:nrow(df)){
summ<-0
for(j in 1:3 ){
summ<-(df[i,j]-df[i,4])^2+summ
}
df$si_sq[i]<-summ/mi-1
}
(apply(df[,1:3],1,sd))^2
df$si_sq<-(apply(df[,1:3],1,sd))^2
df$ti<-apply(df[,1:3],1,sum)
df$ti_hat<-df$yi_bar*Mi
tunb_hat<-(N/n)*sum(df$ti_hat)
df$err<-(apply(df[,1:3],1,sd))^2
si_sq<-sum(df$err)/mi-1
Var_tunb1<-N^2*(1-n/N)*st_sq/n
tbyN<-tunb_hat/N
st_sq<-sum((df$ti_hat-tbyN)^2)/n-1
si_sq<-sum(df$err)/mi-1
Var_tunb1<-N^2*(1-n/N)*st_sq/n
Var_tunb2<-N*(1-mi/Mi)*Mi^2*si_sq/mi
Var_tunb<-Var_tunb1+Var_tunb2
se_tunb<-sqrt(Var_tunb)
se_tunb
K<-N*Mi
install.packages('gtools')
library(gtools)
library(gtools)
M0=2000
t=600
N=4
n=2
df=matrix(nrow=4,ncol=2,c(110,44,265,82,360,112,1265,362),byrow=TRUE)
M=df[,1]
phi=M/M0
w=1/(n*phi)
perm<-permutations(n=4,r=2,v=1:4,repeats.allowed = TRUE)
samp=list()
t_hat=c()
psamp=c()
for(i in 1:nrow(perm)){
indi=perm[i,]
samp[[i]]<-cbind(df[indi,],w[indi])
t_hat[i]<-sum(samp[[i]][,2]*samp[[i]][,3])
psamp[i]<-prod(phi[indi])
}
E_t_hat=sum(t_ht*psamp)
E_t_hat
}
library(gtools)
M0=2000
t=600
N=4
n=2
df=matrix(nrow=4,ncol=2,c(110,44,265,82,360,112,1265,362),byrow=TRUE)
M=df[,1]
phi=M/M0
w=1/(n*phi)
perm<-permutations(n=4,r=2,v=1:4,repeats.allowed = TRUE)
samp=list()
t_hat=c()
psamp=c()
for(i in 1:nrow(perm)){
indi=perm[i,]
samp[[i]]<-cbind(df[indi,],w[indi])
t_hat[i]<-sum(samp[[i]][,2]*samp[[i]][,3])
psamp[i]<-prod(phi[indi])
}
E_t_hat=sum(t_hat*psamp)
E_t_hat
}
V_t_hat<-sum(psamp*(t_hat-E_t_hat)^2)
V_t_hat
n=2
com<-combn(x=1:4,m=2)
samp=list()
y_bar=c()
psamp=1/ncol(com)
for(i in 1:ncol(com)){
ind=com[,i]
samp[[i]]=df[ind,]
y_bar[i]=sum(samp[[i]][,2])/n
}
E_t_hat = N*sum(psamp*y_bar)
E_t_hats
E_t_hat
y_bar_u=mean(df[,2])
s2=1/(N-1)*sum((df[,2]-y_bar_u)^2)
V_t_hat<-N^2*s2*(1-n/N)/n
V_t_hat
install.packages('Rtools')
tk4[1]
tk4<-unique(tk3)
setwd('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5')
library(quanteda)
library(readtext)
library(spacyr)
library(Rcpp)
f<-readtext('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5/data/ground_truth/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
#tk<-tokens(mycorpus,remove_numbers = TRUE,remove_punct = TRUE)
#Extracts bigrams:
#tokens(mycorpus[1],what="character",remove_numbers = TRUE,remove_punct = TRUE,ngrams=2)
# tk2<-tokens_remove(tk,stopwords("english"))
# tk3<-sapply(tk2, tolower)
# summary(tk3)
# texts(tk3)[1]
mastercorpus<-corpus(texts(mycorpus, groups = rep(1, ndoc(mycorpus))))
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tk4[1]
tk4[1,]
is.list(tk4)
r<-as.list(tk4)
r[1]
class(tk4)
dim(tk4)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
View(tkns)
dict<-tkns[tkns$X1.1>1,]
View(dict)
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(max(dict$length)),]
dict[which(dict$length==max(dict$length)),]
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(dict$length==max(dict$length)),]
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(dict$length==max(dict$length)),]
remtk<-gsub("\'", " ", tk)
remtk[10049]
tk3<-sapply(remtk, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(dict$length==max(dict$length)),]
mastercorpus<-corpus(texts(mycorpus, groups = rep(1, ndoc(mycorpus))))
setwd('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5')
library(quanteda)
library(readtext)
library(spacyr)
library(Rcpp)
f<-readtext('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5/data/ground_truth/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
mastercorpus<-corpus(texts(mycorpus, groups = rep(1, ndoc(mycorpus))))
remtk<-gsub("\'", " ", mastercorpus)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(dict$length==max(dict$length)),]
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
# remtk<-gsub("\'", " ", mastercorpus)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
max(dict$length)
dict[which(dict$length==max(dict$length)),]
View(dict)
#creating a list of bigrams:
bi_tkns<-tokens(tk4,what="character",ngrams=2,concatenator='')
bi_tkns[1]
bi_tkns[2]
bi_tkns[3]
bi_tkns[4]
bi_tkns[4,1]
#creating a list of bigrams:
bi_tkns<-tokens(mastercorpus,what="character",ngrams=2,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE,concatenator='')
bi_tkns[1]
bi_tkns[2]
bi_tkns
tokens_remove(bi_tkns,"\\n")
tokens_remove(bi_tkns,"\n")
#creating a list of bigrams:
bi_tkns<-tokens(mastercorpus,what="character",remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE,ngrams=2,concatenator='')
ni_tkns
ni_tokns
bi_tkns
class(bi_tkns)
setwd('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5')
library(quanteda)
library(readtext)
library(spacyr)
library(Rcpp)
f<-readtext('C:/Users/Deepika/Documents/ADS/Project 4/Fall2018-Project4-sec2--sec2proj4_grp5/data/ground_truth/*.txt',cache=FALSE)
mycorpus<-corpus(f)
summary(mycorpus,5)
#tk<-tokens(mycorpus,remove_numbers = TRUE,remove_punct = TRUE)
#Extracts bigrams:
#tokens(mycorpus[1],what="character",remove_numbers = TRUE,remove_punct = TRUE,ngrams=2)
# tk2<-tokens_remove(tk,stopwords("english"))
# tk3<-sapply(tk2, tolower)
# summary(tk3)
# texts(tk3)[1]
txt<-texts(mycorpus, groups = rep(1, ndoc(mycorpus)))
txt<-gsub("\'", " ", txt)
mastercorpus<-corpus(txt)
# remtk<-gsub("\'", " ", mastercorpus)
tk<-tokens(mastercorpus,remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE)
tk2<-tokens_remove(tk,stopwords("english"))
tk3<-sapply(tk2, tolower)
summary(tk3)
length(tk3)
tk4<-unique(tk3)
# #creating a list of bigrams:
# bi_tkns<-tokens(mastercorpus,what="character",remove_numbers = TRUE,remove_punct = TRUE,remove_separators = TRUE,remove_symbols = TRUE,remove_hyphens=TRUE,ngrams=2,concatenator='')
#Putting together the dictionary:
tkns<-data.frame(word=tk4,chars=nchar(tk4))
dict<-tkns[tkns$X1.1>1,]
colnames(dict)<-c('word','length')
#finding the longest word in 100 texts:
l<-max(dict$length)
#check if the max length word makes sense:
dict[which(dict$length==max(dict$length)),]
View(dict)
D12<-matrix(data=0,nrow=26,ncol = 26)
View(D12)
D12[1,1]
char('happy')
split('happy')
strsplit('happy')
split('happy',split='')
split('happy',split="")
x <- "Split at every character."
strsplit(x, "")
strsplit(dict$word[1], "")
}
dict$word[1]
as.character(dict$word[1])
strsplit(as.character(dict$word[1]), "")
match(unlist(chars), letters)
i<-1
chars<-strsplit(as.character(dict$word[i]), "")
match(unlist(chars), letters)
pos1=1
pos2=2
D12<-matrix(data=0,nrow=26,ncol = 26)
for(i in 1:nrow(dict)){
if(dict$length[i]<l){
chars<-strsplit(as.character(dict$word[i]), "")
ind<-match(unlist(chars), letters)
D12[ind[pos1],ind[pos2]]<-1
}
}
View(D12)
l
l*(l-1)
l*(l-1)/2
superlist<-rep(matrix(data=0,nrow=26,ncol = 26),listlen)
listlen<-l*(l-1)/2
superlist<-rep(matrix(data=0,nrow=26,ncol = 26),listlen)
superlist[1]
elm<-matrix(data=0,nrow=26,ncol = 26)
superlist<-rep(list(elm),listlen)
superlist[1]
mat_assignment<-function(pos1,pos2){
D12<-matrix(data=0,nrow=26,ncol = 26)
for(i in 1:nrow(dict)){
if(dict$length[i]<l){
chars<-strsplit(as.character(dict$word[i]), "")
ind<-match(unlist(chars), letters)
D12[ind[pos1],ind[pos2]]<-1
}
return(D12)
}
}
#looping master list:
count=1
listlen<-l*(l-1)/2
elm<-matrix(data=0,nrow=26,ncol = 26)
superlist<-rep(list(elm),listlen)
for(k in 1:l){
for(m in 1:l){
if(k!=m){
superlist[count]<-mat_assignment(k,m)
count=count+1
}
}
}
superlist[1]
superlist[2]
superlist[[2]]
warnings()
#superlist<-rep(list(elm),listlen)
superlist<-rep(0,listlen)
for(k in 1:l){
for(m in 1:l){
if(k!=m){
superlist[count]<-mat_assignment(k,m)
count=count+1
}
}
}
mat_assignment(1,2)
mat_assignment(1,3)
pos1<-1
pos2<-2
D12<-matrix(data=0,nrow=26,ncol = 26)
mat_assignment<-function(pos1,pos2){
D12<-matrix(data=0,nrow=26,ncol = 26)
for(i in 1:nrow(dict)){
if(dict$length[i]<l){
chars<-strsplit(as.character(dict$word[i]), "")
ind<-match(unlist(chars), letters)
D12[ind[pos1],ind[pos2]]<-1
}
}
return(D12)
}
count=1
listlen<-l*(l-1)/2
elm<-matrix(data=0,nrow=26,ncol = 26)
#superlist<-rep(list(elm),listlen)
superlist<-rep(0,listlen)
for(k in 1:l){
for(m in 1:l){
if(k!=m){
superlist[count]<-mat_assignment(k,m)
count=count+1
}
}
}
superlist[1]
